{"cells":[{"cell_type":"markdown","metadata":{},"source":[" # Navigation\n","\n"," ---\n","\n"," In this notebook, you can find all the necessary code for training an agent with an Double DQN Network to learn how to collect yellow and keep away from blue bananas in an [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) environment.\n","\n"," ### 1. Import all necessary packages"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","\n","from unityagents import UnityEnvironment\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from collections import deque\n","import torch\n","from ddqn_agent import Agent\n"]},{"cell_type":"markdown","metadata":{},"source":["### 2. Open Environment and check it out\n","We use the UnityEnvironment `Banana` for Windows 64-bit so make sure you run this notebook on a Windows machine.\n","Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","env = UnityEnvironment(file_name=\"Banana_Windows_x86_64/Banana.exe\")\n","brain_name = env.brain_names[0]\n","brain = env.brains[brain_name]\n","env_info = env.reset(train_mode=True)[brain_name]\n","\n","\n","# number of agents in the environment\n","print('Number of agents:', len(env_info.agents))\n","\n","# number of actions\n","action_size = brain.vector_action_space_size\n","print('Number of actions:', action_size)\n","\n","# examine the state space \n","state = env_info.vector_observations[0]\n","print('States look like:', state)\n","state_size = len(state)\n","print('States have length:', state_size)\n"]},{"cell_type":"markdown","metadata":{},"source":[" ### 2. Examine the State and Action Spaces\n","\n"," The simulation contains a single agent that navigates a large environment.  At each time step, it has four actions at its disposal:\n"," - `0` - walk forward\n"," - `1` - walk backward\n"," - `2` - turn left\n"," - `3` - turn right\n","\n"," The state space has `37` dimensions and contains the agent's velocity, along with ray-based perception of objects around agent's forward direction.  A reward of `+1` is provided for collecting a yellow banana, and a reward of `-1` is provided for collecting a blue banana."]},{"cell_type":"markdown","metadata":{},"source":[" ### 3. Train the agent in the environment\n","\n","In the next step we define our Double-DQN Agent and initialize it with the proper state and action sizes, as well with a random seed."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","agent = Agent(state_size=state_size, action_size=action_size, seed=1337)\n","\n"]},{"source":["Let's define a Deep Q-Learning function to iterate over all episodes and timesteps within that. This function returns a list of the scores of all episodes.\n","Our target score is an average of 13 over the last 100 episodes. But we are optimistic with our Double DQN approach and trry to reach a average score of 16."],"cell_type":"markdown","metadata":{}},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","\n","def dqn(target_score=13 ,n_episodes=2000, max_t=1000, eps_start=1.0, eps_end=0.01, eps_decay=0.995):\n","    \"\"\"Deep Q-Learning.\n","    \n","    Params\n","    ======\n","        n_episodes (int): maximum number of training episodes\n","        max_t (int): maximum number of timesteps per episode\n","        eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n","        eps_end (float): minimum value of epsilon\n","        eps_decay (float): multiplicative factor (per episode) for decreasing epsilon\n","    \"\"\"\n","    scores = []                        # list containing scores from each episode\n","    scores_window = deque(maxlen=100)  # last 100 scores\n","    eps = eps_start                    # initialize epsilon\n","    for i_episode in range(1, n_episodes+1):\n","        env_info = env.reset(train_mode=True)[brain_name]  # reset the environment\n","        state = env_info.vector_observations[0]            # get the current state  \n","        score = 0\n","        for t in range(max_t):\n","            action = agent.act(state, eps).astype(np.int32)\n","            env_info = env.step(action)[brain_name]        # send the action to the environment\n","            next_state = env_info.vector_observations[0]   # get the next state\n","            reward = env_info.rewards[0]                   # get the reward\n","            done = env_info.local_done[0]                  # see if episode has finished\n","            agent.step(state, action, reward, next_state, done)\n","            state = next_state\n","            score += reward\n","            if done:\n","                break \n","        scores_window.append(score)       # save most recent score\n","        scores.append(score)              # save most recent score\n","        eps = max(eps_end, eps_decay*eps) # decrease epsilon\n","        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")\n","        if i_episode % 100 == 0:\n","            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n","        if np.mean(scores_window)>=target_score:\n","            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_window)))\n","            torch.save(agent.qnetwork_local.state_dict(), 'checkpoint.pth')\n","            break\n","    return scores\n","\n"]},{"source":["So now lets run the function to train and save a plot of the score progression. If the agent reaches our target score, it gets automatically saved as a `checkpoint.pth` file."],"cell_type":"markdown","metadata":{}},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["scores = dqn(target_score=16)"]},{"source":["Nice after 500 episodes we already reached our average score of 13.\n","But more impressively we achieved the score of 16 after 849 episodes."],"cell_type":"markdown","metadata":{}},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["env.close()\n","# plot the scores\n","fig = plt.figure(figsize=(10,7))\n","ax = fig.add_subplot(111)\n","plt.plot(np.arange(len(scores)), scores)\n","plt.ylabel('Score')\n","plt.xlabel('Episode #')\n","plt.show()"]},{"source":[" ### 3. See the trained agent in action\n"," Lets open up the environment again and see our agent in action. For that we recreate a agent object and load our `checkpoint.pth` file to validate it is working properly."],"cell_type":"markdown","metadata":{}},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["env = UnityEnvironment(file_name=\"Banana_Windows_x86_64/Banana.exe\")\n","\n","brain_name = env.brain_names[0]\n","brain = env.brains[brain_name]\n","env_info = env.reset(train_mode=False)[brain_name] # reset the environment\n","\n","action_size = brain.vector_action_space_size\n","state = env_info.vector_observations[0]\n","state_size = len(state)\n","\n","agent = Agent(state_size=state_size, action_size=action_size, seed=1337)\n","agent.qnetwork_local.load_state_dict(torch.load('checkpoint.pth'))\n","\n","state = env_info.vector_observations[0]            # get the current state\n","score = 0                                          # initialize the score\n","counter=0\n","while True:\n","    action = agent.act(state).astype(np.int32)        # select an action\n","    env_info = env.step(action)[brain_name]        # send the action to the environment\n","    next_state = env_info.vector_observations[0]   # get the next state\n","    reward = env_info.rewards[0]                   # get the reward\n","    done = env_info.local_done[0]                  # see if episode has finished\n","    score += reward                                # update the score\n","    state = next_state                             # roll over the state to next time step\n","    counter+=1\n","    if done:                                       # exit loop if episode finished\n","        break\n","\n","env.close()\n","print(\"Score: {}\".format(score))\n","print(counter)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.13"},"orig_nbformat":2,"kernelspec":{"name":"python3613jvsc74a57bd0c8395125d241c0f45d9f5d79c5866dad13fd4ae4c33db622cc71cd661dfd4a68","display_name":"Python 3.6.13 64-bit ('drl-p-1': conda)"}}}